{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.8: Tree Based Methods\n",
    "\n",
    "In this case, we will be conduct a simpler exercise with decision trees using previous implementations. In particular, we will make use of several implemented methods in ML libraries s.a. `sklearn` (_that should be good news for you, doesn't it?_). With this, we will try to explore the main characteristics of decision trees, that you will also have to explore in the more theoretical part of the lab (the other exercise, the one on the pdf).\n",
    "\n",
    "We will begin, as usual, importing the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# ML libraries to construct, use and analyse the trees\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "from IPython.display import Image  \n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "import pydotplus\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first apply this to a regression dataset so that you see the way the model is constructed for this case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Tree\n",
    "\n",
    "For starters, let us try out a regression tree. To that end, first load the `Hitters.csv` dataset from the `data` \n",
    "folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters  = # TODO (make sure you remove the None values!)\n",
    "\n",
    "# Print the columns here to check their names\n",
    "print(hitters.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first case, we will only use the variables `Years` and `Hits` for the tree. Our target variable will be `Salary`. Separate them into `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the independent variables (X) from the dependent one (y - salary)\n",
    " \n",
    "X = # TODO\n",
    "y = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a _decision tree regressor_ using the `sklearn` function and fit it. To do that, check out the `DecisionTreeRegressor` in sklearn and implement it here.\n",
    "\n",
    "For reproducibility, fix the `random_state` to `0` and the `max_leaf_nodes` to `3` (make sure you know what this last thing does!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the regressor\n",
    "regressor = # TODO\n",
    "\n",
    "# Fit it with the .fit method\n",
    "regressor.fit(None, None)  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will employ some functions engrained in `StringIO` alongside the method `export_graphviz` from the `tree` object in sklearn. This will enable us to visualize the constructed tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(regressor, \n",
    "                out_file=dot_data, \n",
    "                feature_names=['Years', 'Hits'], \n",
    "                filled=True, \n",
    "                class_names=None)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: Describe the previous tree. What do you see? Do you think this will work well? Respond **briefly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the decision regions using the information on the cuts. Add lines wherever needed so that you can see the decision boundaries for the regression tree above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters.plot('Years', 'Hits', kind='scatter', color='orange', figsize=(7,6))\n",
    "plt.xlim(0,25)\n",
    "plt.ylim(ymin=-5)\n",
    "\n",
    "# Add whatever you may need here to clearly plot the decision boundaries \n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Size\n",
    "\n",
    "Now, for the previous part we limited the growth of the tree so that we recovered a simple (but easily interpretable) tree. Now we will go all-out: we will construct a more exhaustive tree using different variables. For this particular case, let us use **all variables except** `League`, `Division`, `NewLeague` and `Salary` as independent variables to predict, precisely, the `Salary` value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the input variable dataset\n",
    "X = # TODO\n",
    "\n",
    "# Print the column names to check\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform the train/test split, but we will do it so that the proportion of train and test examples is $50\\%$ (that is, train and test consist on $50\\%$ of the datapoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split here. Do it so that the  \n",
    "x_train, x_test, y_train, y_test = train_test_split(None, None, None, None) # TODO: Fill the NAs, fixing also the random_state to 0 for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the tree to its fullest extent: put no limits on the growth and see what happens. You can re-use some of the previous `graphviz` code to visualize the tree here. Plot the complete tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree without limits to its growth (random_state = 0)\n",
    "unlimited_tree_regressor = # TODO \n",
    "unlimited_tree_regressor.fit(None, None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the code you need to plot the tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Question: What do you see? What can you say about this tree? Does it have any important properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we went _a bit too far_ with the tree... Let's set up some limitations to see everything better. Try setting the `max_features` to 9, and the `max_depth` to 4. (_It is important you understand what these parameters do! Check out the documentation in the [library](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_2 =  # TODO (random_state = 0)\n",
    "regressor_2.fit(None, None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the code you may need to plot the tree here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if this tree works well at all... Since we are performing regression, we can obtain the RMSE (we use the Root MSE since it shares the same dimensions of the outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = # TODO: Obtain the predictions for x_test\n",
    "\n",
    "# TODO: Print the RMSE for the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is: how do we know which tree depth to select here? As you may expect, the answer is, as almost always here, performing _cross validation_. In this particular instance we will not conduct exhaustive cross validation. Instead, we will do it in a very simple manner, obtaining *a single tree* for each depth value we want, fitting it to the data and seeing how well does it perform both in train and test  fitted to the data. To do this, do the following:\n",
    "* Fit a **fixed max depth** (`i`) decision tree regressor using *all `x_train` variables*. Also, *fix the `random state` to 1* for reproducibility.\n",
    "* Register its train and test RMSEs\n",
    "* Plot the train and test RMSE curves for each `i` depth  \n",
    "\n",
    "Make sure that you explore _enough_ depth values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the results\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "\n",
    "# Range of depths to be explored\n",
    "tree_size = np.arange(1,20)\n",
    "\n",
    "for i in tree_size:\n",
    "\n",
    "    # TODO: Train the needed tree with the set depth, then measure its RMSE in train and test and store them in the previous lists\n",
    "    \n",
    "\n",
    "# Plot the results\n",
    "plt.plot(tree_size, train_rmse, 'r*--')\n",
    "plt.plot(tree_size, test_rmse, 'b.-')\n",
    "plt.xlabel('Max_depth')\n",
    "plt.ylabel('Root Mean Squared Error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: What do you see here? What depth value would you select?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not really making CV, we do not have multiple values for the train and test RMSE for each tree. Therefore, we *do not* have errorbars in the previous plot. That should raise some suspicions from your part. \n",
    "\n",
    "> Question: What happens if we change the `random_state` value? Are the previous results robust?\n",
    "\n",
    "To answer the previous question you can try out code in the next cell. Feel free to try whatever you think is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more acquainted with the results, check out what happens if you include less  `x_train` features. To do so, change what you need from the previous block of code and put it in the next block here.\n",
    "> Question: Do you see any important changes? How do you explain this?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete this practical exercises with trees, we will also try out some classification trees to later do ensembles. Let us see how this works. \n",
    "\n",
    "First, load the `Carseat.csv` dataset from `data` (make sure to remove the NAs, as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (remove the NAs)\n",
    "carseats = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it so that we have a new binary variable called `high`. This variable should be `1` when `sales` are over 8, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats['high'] = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the remaining variables to make them usable here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `ShelveLoc`, `Urban` and `US` need to be converted to categorical variables to be correctly used. To that end, I suggest you use `pd.factorize` (although feel free to do as you will here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will employ all variables to predict the `high` value (except `Sales` and `high`, for obvious reasons). Note that we have essentially converted a _regression_ problem into a _binary classification_ one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  # TODO: Drop the unwanted columns\n",
    "y =  # TODO:  Select the \"high\" column\n",
    "\n",
    "# TODO: Performn the train/test split with again 50% data for train and 50% for test \n",
    "X_train, X_test, y_train, y_test= train_test_split(None, None, None, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a decision tree classifier. To control for the depth, we will fix it to a *maximum depth of 6*. Use as impurity criteria the **Gini index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carseats_classifier = DecisionTreeClassifier(None, None)  # TODO: Fill the NAs. Fix the random_state to 0\n",
    "\n",
    "# Train the model with .fit\n",
    "carseats_classifier.fit(None, None) # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the tree (again, reuse whatever you may need here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now assess the quality of the tree. To that end, **represent the confusion matrix** for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Questions: \n",
    "> * What is the **precision** of this tree?\n",
    "> * Do you consider the dataset balanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "Now we will try out some of the ensemble methods from class. Remember there is an stochastic component embedded in these for the most part, so we may not recover exactly the same results twice depending on how you implement things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree models mentioned above usually suffers from high variance. **B**ootstrap **agg**regation, or **bagging** usually helps with this issue. To do bagging here, we will do it both by hand and by employing the sklearn function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's go with the *by-hand* implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bagging parameters\n",
    "n_estimators = 10  # Number of decision trees in the ensemble\n",
    "max_samples = 0.8  # Proportion of samples to be used for each bootstrap sample\n",
    "\n",
    "# Store the predictions\n",
    "predictions = []\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "\n",
    "    # Create a bootstrap sample\n",
    "    sample_indices = # TODO\n",
    "    X_bootstrap = # TODO \n",
    "    y_bootstrap = # TODO \n",
    "    \n",
    "    # Train a decision tree classifier on the bootstrap sample\n",
    "    decision_tree = # TODO (impose no restrictions whatsoever)\n",
    "    decision_tree.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    # Make predictions on the test set using the trained decision tree\n",
    "    y_pred = # TODO: Obtain the predictions for X_test\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Combine predictions\n",
    "# For classification, you can use majority voting\n",
    "majority_vote = # TODO: Obtain the majority vote for each instance\n",
    "\n",
    "# For regression, you can use averaging\n",
    "# combined_predictions = np.(predictions, axis=0)  # For classification, use np.mean for voting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here! Use majority_vote and y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this is done in `sklearn`... Fit it and show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = # TODO: Create a BaggingClassifier (fix random_state to 0)\n",
    "\n",
    "# TODO: Train it with the training data\n",
    "bagging.fit(None, None)\n",
    "\n",
    "# TODO: Obtain the predictions\n",
    "bagging_pred = bagging.predict(None)\n",
    "\n",
    "# TODO: Print the confusion matrix (use the confusion_matrix function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `sklearn` implementation to study the variable importance. *Make sure you understand how this is done!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_featureImportance = pd.DataFrame({'Feature Importance':bagging.feature_importances_*100},index = X.columns)\n",
    "bagging_featureImportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: What do you see here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We will also do this in the RF case. First, we will implement it by hand. Feel free to use the previous code and modify it as you may see fit to do RF here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RF parameters\n",
    "n_estimators = 10  # Number of decision trees in the forest\n",
    "max_features = 0.8  # Proportion of features to consider for each split\n",
    "\n",
    "# Train decision trees with random feature selection and make predictions\n",
    "predictions = []\n",
    "for _ in range(n_estimators):\n",
    "\n",
    "    # TODO: Construct your own RF ensemble! Reuse the Bagging code and change whatever you may need here\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "\n",
    "\n",
    "\n",
    "# TODO: Finally, combine predictions using majority voting\n",
    "majority_vote = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do it again with `sklearn` so you see the differences... Show the confusion matrix. In this last part there may be some differences with your run, which are due to the randomness of the classifiers constructed. Do not worry too much about it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Construct the RF classifier with RandomForestClassifier. Fix the random_state to 0, n_estimators to 10 and max_features to 0.8\n",
    "rf = # TODO\n",
    "\n",
    "# TODO: Train it\n",
    "rf.fit(None, None)\n",
    "\n",
    "# TODO: Predict the test values\n",
    "rf_pred= rf.predict(None)\n",
    "\n",
    "# TODO: Obtain the confusion matrix and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using again the `sklearn` implementation, we will study the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_featureImportance= pd.DataFrame({'Feature Importance':rf.feature_importances_*100}, index= X.columns)\n",
    "rf_featureImportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Question: Given the models thus far (simple tree, bagging and RF), which one would you choose and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Boosting\n",
    "\n",
    "In order to fully complete our review of the ensemble methods from class, we are missing the **Boosting method**. In order to keep matters simple, we will implement it here with `sklearn` so that you get to see what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create the Boosting model\n",
    "gb= GradientBoostingClassifier(n_estimators = 5000, random_state = 1, max_depth = 2)\n",
    "\n",
    "# If you want to try it out, you can change reuse most of previous codes to run it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty good! Keep in mind that this is achieved with super weak learners s.a. trees with depth 2. It is quite fast, and super easy to use with `sklearn`. We can also study the variable importance in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_featureimportance= pd.DataFrame({'Feature Importance': gb.feature_importances_*100}, index= X.columns)\n",
    "gb_featureimportance.sort_values('Feature Importance', ascending=False).plot(kind='bar', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see how this can be implemented easily by hand, you can use the following code. We are using an implementation that follows a description of Boosting mode similar to the one given in the ISLR book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "n_estimators = 1000  # Number of decision trees in the ensemble\n",
    "learning_rate = 0.1  # Learning rate for each decision tree\n",
    "\n",
    "# Initialize the weights for the training samples\n",
    "sample_weights = np.ones(len(X_train)) / len(X_train)\n",
    "\n",
    "# Train decision trees with weighted samples and make predictions\n",
    "predictions = []\n",
    "for _ in range(n_estimators):\n",
    "\n",
    "    # Train a decision tree classifier on the weighted training samples\n",
    "    decision_tree = DecisionTreeClassifier(max_depth = 2)\n",
    "    decision_tree.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    # Make predictions on the test set using the trained decision tree\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    # Calculate error\n",
    "    incorrect = (y_pred != y_test).astype(int)\n",
    "    error = np.sum(sample_weights * incorrect) / np.sum(sample_weights)\n",
    "\n",
    "    # Update sample weights\n",
    "    alpha = learning_rate * np.log((1 - error) / error)\n",
    "    sample_weights *= np.exp(alpha * incorrect)\n",
    "\n",
    "# Combine predictions\n",
    "# For classification, you can use weighted voting\n",
    "combined_predictions = np.zeros(len(X_test))\n",
    "for prediction in predictions:\n",
    "    combined_predictions += prediction\n",
    "\n",
    "# Evaluate the performance of the boosting ensemble\n",
    "# TODO: This can be done with the previous codes you had already. Reuse them here to see the result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
